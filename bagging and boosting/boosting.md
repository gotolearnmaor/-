#    今天来说说boosting

简单地来说，提升就是指每一步我都产生一个弱预测模型，然后加权累加到总模型中，然后每一步弱预测模型生成的的依据都是损失函数的负梯度方向，这样若干步以后就可以达到逼近损失函数局部最小值的目标。 梯度提升中，不同于bagging,在梯度提升中，无法并行训练模型

## 1.1 GBDT 梯度提升决策树（Gradient Boosting Decision Tree）
我们训练模型的目的是使得损失函数最小化，而利用多个模型的目的也是使得损失函数最小化，但是要同时优化多个模型的想法难以实现。于是一个折中的思路被提了出来，每次只优化一个模型，下一个模型尽可能的接近该模型的误差值，这样，一步一步我们不是距离结果更近了吗。 由此，梯度提升算法应运而生。

梯度提升决策树的核心思想可以用一条公式概况（又进入不说人话环节了）

![image](https://github.com/gotolearnmaor/ML-a-long-way/blob/master/image/20170309121033772.png)  
  
根据上述公式，L()代表损失函数，要使得损失函数尽可能的小，由梯度下降法给我们提供的思路，我们只需要在负梯度上乘一个学习率，就可以使得损失函数不断减小，这也是GBDT的核心思想，即用一个回归树，来学习负梯度

![image](https://github.com/gotolearnmaor/ML-a-long-way/blob/master/image/20170309121055960.png)

## 1.2 算法步骤

1.始化一棵树，做回归，计算其残差值  

2.计算负梯度（即残差值），将其作为样本的Lable进行新的回归树的训练，目的是拟合这个残差值（记得设置一个学习率）

3、不断更新学习器，得到输出的最终模型 f(x)


## 2 xgboost

XGBoost算法的基本思想与GBDT类似，不断地地进行特征分裂来生长一棵树，每一轮学习一棵树，其实就是去拟合上一轮模型的预测值与实际值之间的残差。当我们训练完成得到k棵树时，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需将每棵树对应的分数加起来就是该样本的预测值  

既然同样是学习残差值，那么区别在哪里？？最主要的差别在于损失函数的不同，因此学习的残差也不同。

首先来说说xgboost中的弱学习器，xgboost选择了cart决策树，即基于基尼系数的决策树，但是基尼系数无法计算平方损失，也就无法进行GB操作了。
关于xgboost，公式推导我也搞不明白...有数学能力强的大佬还望指点。我只说说他的思想吧，在GBDT中对损失函数求梯度，下一个学习器学习的是一个负梯度，但是对于除了平方损失函数之外的一些其他损失函数，往往梯度计算复杂，因此呢可以通过泰勒二阶展开，这样对于其他损失函数，求解就会方便很多。
因此在xgboost中，最终学习目标是依赖于每个数据点的在误差函数上的一阶导数和二阶导数
XGBOOST的优点：  
-支持线性分类器   
-可以自定义损失函数，并且可以用二阶偏导  二阶导数可以使得梯度下降更快！！（二阶导数就是梯度的梯度） 
-加入了正则化项：叶节点数、每个叶节点输出 score 的 L2-norm  （将树的复杂度加入正则项）
-支持特征抽样  
-在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。
-节点分裂方式不同：xgboost节点：第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失

## 3 lightgbm
xgboost已经足够强大，但缺点依然明显，每次在计算一个分裂点的时候，都会遍历数据集，如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的


1.相对于预排序，LightGBM使用了直方图算法来减少存储所消耗的资源  
将很多的数据进行离散化，相当于区间统计，这样减少了数据量，比如离散化成了K个整数，遍历计算分裂点的时候不需要再遍历整个数据，而只需要遍历k个即可  

2.带深度限制的Leaf-wise的叶子生长策略   
不是每个节点都分裂，只选择增益最大的分裂  

3.直方图做差加速
节点直方图减去左孩子直方图可以直接得到右孩子直方图

4.直接支持类别特征(Categorical Feature)  
5.Cache命中率优化  
6.基于直方图的稀疏特征优化  
多线程优化  






