#                               bagging

## 前言

随着我们训练任务的不断提升，以及对于预测准确的要求的提高，传统的弱学习器（单一模型）已经越来越难以满足  
我们的需求，为了寻求新的突破，前人提出了集成学习（ensemble learing）的思想。所谓集成学习，就是用多个  
弱学习器去共同学习，得出结果。  
## 1.1 bagging 原理

 bagging 可以看作一种弱学习器的组装方法，通过多个弱学习器组合，得到一个强的学习器  
 
1.从原始样本集中随机采样。每轮从原始样本集中有放回的选取n个训练样本（在训练集中，有些样本可能被多次抽取到，  
  而有些样本可能一次都没有被抽中），每次抽取都会得到一个训练集，这便是bagging用于训练不同学习器的训练集。 
  
2.我们假设一共需要训练k个弱学习器，则需要共进行k轮抽取，得到k个训练集。（bootstrap的过程，由于是有放回  
  抽样，所以k个训练集之间相互独立）  
  
3.我们可以选择不同的弱学习器进行训练，每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型。  
（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）  

4.利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。(aggregating的过程)  

5.最终结果  
  分类问题：将上步得到的k个模型采用投票的方式得到分类结果  
  回归问题：计算上述模型的均值作为最后的结果。（所有模型的重要性相同）  

##  1.2 bagging 的主要特点

1.每个模型独立，可以并行计算，训练速度快  

2.减小方差（variance）,对于选择性偏差（bais）影响很小

<img src='image/CodeCogsEqn.png'>

所以 Bagging后的偏差与单个模型相近。




