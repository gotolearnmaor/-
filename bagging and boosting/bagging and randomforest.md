#                               bagging

## 前言

随着我们训练任务的不断提升，以及对于预测准确的要求的提高，传统的弱学习器（单一模型）已经越来越难以满足  我们的需求，为了寻求新的突破，前人提出了集成学习（ensemble learing）的思想。所谓集成学习，就是用多个  弱学习器去共同学习，得出结果。  

## 1.1 bagging 原理

 bagging 可以看作一种弱学习器的组装方法，通过多个弱学习器组合，得到一个强的学习器  
 
1.从原始样本集中随机采样。每轮从原始样本集中有放回的选取n个训练样本（在训练集中，有些样本可能被多次抽取到，  而有些样本可能一次都没有被抽中），每次抽取都会得到一个训练集，这便是bagging用于训练不同学习器的训练集。 
  
2.我们假设一共需要训练k个弱学习器，则需要共进行k轮抽取，得到k个训练集。（bootstrap的过程，由于是有放回  抽样，所以k个训练集之间相互独立）  
  
3.我们可以选择不同的弱学习器进行训练，每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型。  （注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）  

4.利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。(aggregating的过程)  

5.最终结果  
  分类问题：将上步得到的k个模型采用投票的方式得到分类结果  
  回归问题：计算上述模型的均值作为最后的结果。（所有模型的重要性相同）  

##  1.2 bagging 的主要特点

1.每个模型独立，可以并行计算，训练速度快  

2.减小方差（variance）,对于选择性偏差（bais）影响很小  
                   ![image](https://github.com/gotolearnmaor/ML-a-long-way/blob/master/image/CodeCogsEqn.png)

  所以 Bagging后的偏差与单个模型相近。  
  
  对于方差有：  
  ![image](https://github.com/gotolearnmaor/ML-a-long-way/blob/master/image/CodeCogsEqn%20(1).gif)  
  由此可以看出，bagging适用于方差较大，偏差较小的样本数据
## 1.3 bagging 的经典算法：随机森林

随机森林是由很多决策树构成的，不同决策树之间没有关联。  

当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。
  
随机森林的基本步骤如下：

1.我们需要构建样本数为n的样本，则有放回的从所有样本集N中随机选择n个样本(一个样本可能被多次选择，也可能不被选择)。选择好的N个样本作为一组训练集用来训   练一个决策树，即作为决策树根节点处的样本。（第一步关键，随机样本）  

2.当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M（即不对所有特征进行判断，而是选择部分特征）。   然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。（第二步关键，随机特征）   

3.决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节   点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。   

4.按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

## 1.4 优缺点
 优点 
 
-处理高维度数据，并且不用降维，无需做特征选择    
-它可以判断特征的重要程度  
-可以判断出不同特征之间的相互影响  
-不容易过拟合  
-训练速度比较快，容易做成并行方法  
-实现起来比较简单  
-对于不平衡的数据集来说，它可以平衡误差。  
-如果有很大一部分的特征遗失，仍可以维持准确度。  

缺点  

-随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合  
-对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的  
 
## 1.5 随机森林 评估特征的方法

### 1 基于基尼系数
决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益（基尼不纯度,是指将来自集合中的某种结果随机应用在集合中，某一数据项的预期误差率）。  

对于回归问题，直接使用方差作为评判标准，即当前节点训练集的方差 Var 减去左节点的方差 VarLeft 和右节点的方差 VarRight

### 2 基于袋外数据
我们用 OOB（out of bag） 样本可以得到测试误差 1；然后随机改变 OOB 样本的第 j 列：保持其他列不变，对第 j 列进行随机的上下置换，得到误差 2。至此，我们可以用误差 1-误差 2 来刻画变量 j 的重要性。基本思想就是，如果一个变量 j 足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要


## 1.6 其他补充

-随机森林树和树之间无依赖性  

-随机森林对异常值不敏感  

-随机森林特征不需要归一化  

-增加树的深度可能会造成过拟合

-随机森林的OOB：  

没有参加决策树建立的数据称为袋外数据 OOB（out of bag）,它可以用于取代测试集,计算决策树的误差  

对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为 O,用这O 个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出 O个数据相应的分类,因为这 O 条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为 X,则袋外数据误差大小=;这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计  

-缺失值处理  

用中位数 众数等进行填充

rfImpute方法 


然后使用上述填补后的训练集来训练随机森林模型，并统计相似度矩阵（proximity matrix），然后再看之前缺失值的地方，如果是分类变量，则用没有缺失的观测实例的相似度中的权重进行投票；如果是连续性变量，则用相似度矩阵进行加权求均值。
上述投票方案迭代进行4~6次。

相似度矩阵就是任意两个观测实例间的相似度矩阵，原理是如果两个观测实例落在同一棵树的相同节点次数越多，则这两个观测实例的相似度越高（主要思想是观察两样本落在同一节的次数）





