#                               bagging

## 前言

随着我们训练任务的不断提升，以及对于预测准确的要求的提高，传统的弱学习器（单一模型）已经越来越难以满足  我们的需求，为了寻求新的突破，前人提出了集成学习（ensemble learing）的思想。所谓集成学习，就是用多个  弱学习器去共同学习，得出结果。  

## 1.1 bagging 原理

 bagging 可以看作一种弱学习器的组装方法，通过多个弱学习器组合，得到一个强的学习器  
 
1.从原始样本集中随机采样。每轮从原始样本集中有放回的选取n个训练样本（在训练集中，有些样本可能被多次抽取到，  而有些样本可能一次都没有被抽中），每次抽取都会得到一个训练集，这便是bagging用于训练不同学习器的训练集。 
  
2.我们假设一共需要训练k个弱学习器，则需要共进行k轮抽取，得到k个训练集。（bootstrap的过程，由于是有放回  抽样，所以k个训练集之间相互独立）  
  
3.我们可以选择不同的弱学习器进行训练，每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型。  （注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）  

4.利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。(aggregating的过程)  

5.最终结果  
  分类问题：将上步得到的k个模型采用投票的方式得到分类结果  
  回归问题：计算上述模型的均值作为最后的结果。（所有模型的重要性相同）  

##  1.2 bagging 的主要特点

1.每个模型独立，可以并行计算，训练速度快  

2.减小方差（variance）,对于选择性偏差（bais）影响很小  
                   ![image](https://github.com/gotolearnmaor/ML-a-long-way/blob/master/image/CodeCogsEqn.png)

  所以 Bagging后的偏差与单个模型相近。  
  
  对于方差有：  
  ![image](https://github.com/gotolearnmaor/ML-a-long-way/blob/master/image/CodeCogsEqn%20(1).gif)  
  由此可以看出，bagging适用于方差较大，偏差较小的样本数据
## 1.3 bagging 的经典算法：随机森林

随机森林是由很多决策树构成的，不同决策树之间没有关联。  

当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。
  
随机森林的基本步骤如下：

我们需要构建样本数为N的样本，则有放回的随机选择N个样本(一个样本可能被多次选择，也可能不被选择)。选择好的N个样本作为一组训练集用来训练一个决策树，即作为决策树根节点处的样本。（第一步关键，随机样本）  
当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M（即不对所有特征进行判断，而是选择部分特征）。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。（第二步关键，随机特征）   

决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。   

按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

## 1.4 优缺点
 优点 
 
-处理高维度数据，并且不用降维，无需做特征选择    
-它可以判断特征的重要程度  
-可以判断出不同特征之间的相互影响  
-不容易过拟合  
-训练速度比较快，容易做成并行方法  
-实现起来比较简单  
-对于不平衡的数据集来说，它可以平衡误差。  
-如果有很大一部分的特征遗失，仍可以维持准确度。  

缺点  

-随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合  
-对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的  
 


